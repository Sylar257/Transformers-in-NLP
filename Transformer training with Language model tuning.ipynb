{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# transformers\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "\n",
    "# our models of choice\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig , RobertaForMaskedLM\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
    "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastai version : 1.0.59\n",
      "transformers version : 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import fastai\n",
    "import transformers\n",
    "print('fastai version :', fastai.__version__)\n",
    "print('transformers version :', transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers model Zoo\n",
    "Create a dictionary of parameters required for creating different model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
    "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
    "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
    "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig, RobertaForMaskedLM),\n",
    "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make model selection here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seed = 42\n",
    "use_fp16 = False\n",
    "bs = 16\n",
    "\n",
    "model_type = 'roberta'\n",
    "pretrained_model_name = 'roberta-base'\n",
    "\n",
    "# model_type = 'bert'\n",
    "# pretrained_model_name='bert-base-uncased'\n",
    "\n",
    "# model_type = 'distilbert'\n",
    "# pretrained_model_name = 'distilbert-base-uncased'\n",
    "\n",
    "#model_type = 'xlm'\n",
    "#pretrained_model_name = 'xlm-clm-enfr-1024'\n",
    "\n",
    "#model_type = 'xlnet'\n",
    "#pretrained_model_name = 'xlnet-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, config_class , LM_model= MODEL_CLASSES[model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base', 'roberta-base-openai-detector', 'roberta-large-openai-detector'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the classification model\n",
    "model_class.pretrained_model_archive_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base', 'roberta-base-openai-detector', 'roberta-large-openai-detector'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the language model\n",
    "LM_model.pretrained_model_archive_map.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the same randomization seed so as to compare different models more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "The important thing here is that `FastAI` uses **processors** to perform repeatetive tasks when creating `DataBunch`. A set of default **processors** are performed for fastai.textlearners. For example:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAI use various processors to perform repeatative tasks in data pipeline\n",
    "\n",
    "def _get_processor(tokenizer:Tokenizer=None, vocab:Vocab=None, chunksize:int=10000, max_vocab:int=60000,\n",
    "                   min_freq:int=2, mark_fields:bool=False, include_bos:bool=True, include_eos:bool=False):\n",
    "    return [TokenizeProcessor(tokenizer=tokenizer, chunksize=chunksize, \n",
    "                              mark_fields=mark_fields, include_bos=include_bos, include_eos=include_eos),\n",
    "            NumericalizeProcessor(vocab=vocab, max_vocab=max_vocab, min_freq=min_freq)]\n",
    "\n",
    "class NumericalizeProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that numericalizes the tokens in `ds`.\"\n",
    "    def __init__(self, ds:ItemList=None, vocab:Vocab=None, max_vocab:int=60000, min_freq:int=3):\n",
    "        vocab = ifnone(vocab, ds.vocab if ds is not None else None)\n",
    "        self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq\n",
    "\n",
    "    def process_one(self,item): return np.array(self.vocab.numericalize(item), dtype=np.int64)\n",
    "    def process(self, ds):\n",
    "        if self.vocab is None: self.vocab = Vocab.create(ds.items, self.max_vocab, self.min_freq)\n",
    "        ds.vocab = self.vocab\n",
    "        super().process(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to overwrite the \"tokenizer\" and \"numericalizer\" in order to tailor the databunch creating process to the `Transformers`\n",
    "Later when creating `DataBunch`, we are going to pass in our customized processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Step 1: create a new tokenizer that inherit from `fastai`'s `BaseTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersBaseTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n",
    "        self._pretrained_tokenizer = pretrained_tokenizer\n",
    "        self.max_seq_len = pretrained_tokenizer.max_len\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def __call__(self, *args, **kwargs): \n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
    "        CLS = self._pretrained_tokenizer.cls_token\n",
    "        SEP = self._pretrained_tokenizer.sep_token\n",
    "        if self.model_type in ['roberta']:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
    "        else:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
    "        return [CLS] + tokens + [SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Get the 'pre-trained tokenizer' from `transformer` library.(this is used for initialization of the class we created above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Create the actual **tokenizer** for our transformer of choice and put a `FastAI` wrapper on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.text.transform.Tokenizer"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
    "fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, be carefull about 3 things :\n",
    "\n",
    "1. As we are not using RNN, we have to limit the sequence length to the model input size.\n",
    "2. Most of the models require special tokens placed at the beginning and end of the sequences.\n",
    "3. Some models like RoBERTa require a space to start the input string. For those models, the encoding methods should be called with add_prefix_space set to True.\n",
    "\n",
    "Below, you can find the resume of each pre-process requirement for the 5 model types used in this tutorial. You can also find this information on the [HuggingFace documentation](https://huggingface.co/transformers/) in each model section.<br>\n",
    "`bert:       [CLS] + tokens + [SEP] + padding`<br>\n",
    "`roberta:    [CLS] + prefix_space + tokens + [SEP] + padding`<br>\n",
    "`distilbert: [CLS] + tokens + [SEP] + padding`<br>\n",
    "`xlm:        [CLS] + tokens + [SEP] + padding`<br>\n",
    "`xlnet:      padding + [CLS] + tokens + [SEP]`\n",
    "\n",
    "It is worth noting that we don't add padding in this part of the implementation.  As we will see later, fastai manage it automatically during the creation of the `DataBunch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizer\n",
    "In `fastai` library, `NumericalizeProcessor` object takes as `bocab` argument a `Vocab` object. Here we will create a new class `TransformerVocab` that inherits from `Vocab` and overwrite `numericalize` and `textify` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersVocab(Vocab):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        super(TransformersVocab, self).__init__(itos = [])\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
    "        \"Convert a list of tokens `t` to their ids.\"\n",
    "        return self.tokenizer.convert_tokens_to_ids(t)\n",
    "        #return self.tokenizer.encode(t)\n",
    "\n",
    "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
    "        \"Convert a list of `nums` to their tokens.\"\n",
    "        nums = np.array(nums).tolist()\n",
    "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize numericalize processor\n",
    "Notice taht we need to pass the `include_bs=False` and `include_eos=False` options. This is because `fastiai` adds its own special tokens by default which interferes with the `[CLS]` and `[SEP]` tokens that are required for RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.NumericalizeProcessor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NumericalizeProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use RoBERTa tokenizer to initialize the `TransformersVocab\n",
    "transformer_vocab = TransformersVocab(tokenizer=transformer_tokenizer)\n",
    "\n",
    "# create a customized numericalizor using vocab just created\n",
    "numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together our customized processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.text.data.TokenizeProcessor"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TokenizeProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function over the tokenizer we created above\n",
    "tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc(TextList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally put the two processors in a list for later use\n",
    "transformer_processor = [OpenFileProcessor(),tokenize_processor, numericalize_processor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataBunch\n",
    "For the DataBunch creation, you have to pay attention to set the processor argument to our new custom processor `transformer_processor` and manage correctly the padding.\n",
    "\n",
    "As mentioned in the [HuggingFace documentation](https://huggingface.co/transformers/), BERT, RoBERTa, XLM and DistilBERT are models with absolute position embeddings, so it's usually advised to pad the inputs on the right rather than the left. Regarding XLNET, it is a model with relative position embeddings, therefore, you can either pad the inputs on the right or on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are using Google XLNet\n",
    "pad_first = bool(model_type in ['xlnet'])\n",
    "pad_idx = transformer_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'Ġcat', 'Ġdidnt', 'Ġjump', 'Ġonto', 'Ġthe', 'Ġtable', ',', 'Ġbecause', 'Ġits', 'Ġtired']\n",
      "[627, 4758, 46405, 3704, 2500, 5, 2103, 6, 142, 63, 7428]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'Ġcat',\n",
       " 'Ġdidnt',\n",
       " 'Ġjump',\n",
       " 'Ġonto',\n",
       " 'Ġthe',\n",
       " 'Ġtable',\n",
       " ',',\n",
       " 'Ġbecause',\n",
       " 'Ġits',\n",
       " 'Ġtired']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = transformer_tokenizer.tokenize('the cat didnt jump onto the table, because its tired')\n",
    "print(tokens)\n",
    "ids = transformer_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "transformer_tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the full IMDB dataset and inspect the files under that path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/projectx/.fastai/data/imdb')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/projectx/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/lm_databunch_RoBERTa'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/test'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/data_bunch_classification'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/imdb_textlist_classifier'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/fine_tuning_LM'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/tmp_lm'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/train'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/lm_databunch'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/tmp_clas'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/README'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/unsup')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/projectx/.fastai/data/imdb/train/unsupBow.feat'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/train/pos'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/train/labeledBow.feat'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/train/neg')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/projectx/.fastai/data/imdb/test/pos'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/test/labeledBow.feat'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/test/neg')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'test').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for LM training\n",
    "The reviews are in a training and test folder. In addition, we have a 'unsup' folder that contains many reviews are \n",
    "are **not labelled**.<br>\n",
    "In the second training phase of ULMFiT, as mentioned in the [README](https://github.com/Sylar257/Sentiment-Analysis/blob/master/README.md)\n",
    ", we will be fine-tuning the language model to the domain-specific corpus.<br>\n",
    "Thus, training could take advantage of using 'train', 'test', and 'unsup' folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataBunch for language model\n",
    "What does that mean?<br>\n",
    "Basically we are creating a databunch for our learner later in a way that we ignore the true label('negataive' or  'positive'). Hence, the task we constructed is as simple as to just predict the **next word** without worrying about what the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(TextList.label_for_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can play around with the batch_size as long as the GPU can take it\n",
    "bs = 32\n",
    "data_lm = (TextList.from_folder(path, processor=transformer_processor) # specify the path\n",
    "           .filter_by_folder(include=['train','test','unsup']) # exclude other folders\n",
    "           .split_by_rand_pct(0.1, seed=seed)                  # randomly split and keep 10% for validation set\n",
    "           .label_for_lm()                                     # label as to \"predict the next word token\"\n",
    "           .databunch(bs=bs))                                  # convert to databunch for the learner later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 10000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_lm.train_ds), len(data_lm.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ĠAside Ġfrom Ġthe Ġfact Ġthat Ġthis Ġmovie Ġwas Ġfilmed Ġmostly Ġin ĠRock port ĠMA , Ġwhich Ġis Ġa Ġbeautiful Ġtown Ġwhere Ġmy Ġmother Ġonce Ġrented Ġa Ġsmall Ġstorefront Ġand ĠI Ġspent Ġmany Ġa Ġpleasant Ġsummer Ġas Ġa Ġchild , Ġit Ġis Ġfun Ġand Ġcute Ġlittle Ġfilm . Ġ< br Ġ/ >< br Ġ/> I Ġmust Ġadmit Ġthat ĠI Ġhad Ġno Ġdesire Ġto Ġactually Ġsee Ġthis Ġmovie Ġeven Ġthough ĠI Ġhave Ġa Ġweak spot Ġfor Ġromantic Ġcomed ies Ġ( I Ġdon 't Ġknow Ġwhy ). ĠThe Ġtrailers ĠI Ġsaw Ġwere Ġnot Ġappealing , Ġthe Ġcast Ġdid Ġnot Ġlook Ġthat Ġinteresting Ġand ĠI Ġhad Ġno Ġidea Ġwhat Ġthe Ġplot Ġwould Ġbe Ġabout . ĠIn Ġthe Ġend ĠI Ġfound Ġit Ġto Ġbe Ġan Ġinteresting Ġmeditation Ġon Ġrelationships Ġand Ġfamily . ĠI Ġthoroughly Ġenjoyed Ġmyself Ġand Ġmust Ġadmit Ġthat ĠI Ġthought Ġthat Ġthis Ġfilm Ġwas Ġone Ġof Ġthe Ġmost Ġoverlooked Ġgems Ġof Ġlast Ġyear . ĠI Ġam Ġdisappointed Ġthat Ġso Ġfew Ġpeople Ġseemed Ġto Ġhave Ġenjoyed Ġthe Ġvery Ġ\" human - ness \" Ġthat Ġthis Ġmovie Ġpresented Ġthe Ġviewer Ġwith .< br Ġ/ >< br Ġ/> I Ġhave Ġread Ġmany Ġbad Ġreviews Ġof Ġthis Ġfilm , Ġand Ġmust Ġadmit Ġa Ġcertain Ġlevel Ġof Ġshock Ġat Ġthe Ġcynicism Ġthat Ġis Ġprevalent Ġin Ġthem . ĠAs Ġa Ġgrad Ġstudent ĠI Ġconsider Ġmyself Ġto Ġbe Ġquite Ġcynical , Ġbut Ġthis Ġwas Ġa Ġbeautiful Ġlittle Ġfilm Ġthat Ġdeserves Ġmuch Ġbetter Ġthan Ġit Ġgot . </s>\n"
     ]
    }
   ],
   "source": [
    "print(data_lm.train_ds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a peek at a small sample of data\n",
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save our databunch so that we don't have to re-run this the next time\n",
    "data_lm.save('lm_databunch_RoBERTa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning Language Model\n",
    "Important things to note here: we can't tune the language model of RoBERTa exactly the same way as ULMFiT AWD_LSTM.\n",
    "The reason is because they use different training strategies. For `AWD_LSTM`, we set up language model training as predicting the **next word** or the **previous word** depending the forward/backward model that we are building. Whereas for `RoBERTa`, we need to fine-tune with masked language modeling strategy which is adopted by most of the **BERT model family** for their bidirectional mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import re\n",
    "import argparse\n",
    "from typing import Tuple\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grab all seperate text files from train/test/unsup folders\n",
    "Now all of the text data are under the `.fastai/data/imdb` directory by default.\n",
    "In particular, there are 5 folders that we are leverage: *`train/pos ; train/neg ; test/pos ; test/neg ; unsup`*\n",
    "The plan is to grab all IMDb text data that we can get our hands on and split then 90% for training language model and 10% for validation.\n",
    "Note that it's better to shuffle our file glob here so that we don't get all validation text data from, say, only the `test/neg` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# combine all the .txt files in the train/pos and train/neg folders\n",
    "train_files = glob.glob(os.path.join((path/'train/*'),'*.txt'))\n",
    "random.shuffle(train_files)\n",
    "\n",
    "# split randomly 90% data to train_data_LM file and 10% to test_data_LM file\n",
    "with open('train_data_LM.txt', 'ab') as outfile:\n",
    "    for f in train_files[:int(len(train_files)*0.9)]:\n",
    "        with open(f, 'rb') as infile:\n",
    "            outfile.write(infile.read())\n",
    "with open('test_data_LM.txt',  'ab') as outfile:\n",
    "    for f in train_files[int(len(train_files)*0.9):]:\n",
    "        with open(f, 'rb') as infile:\n",
    "            outfile.write(infile.read())\n",
    "            \n",
    "# combine all the .txt files in the test/pos and test/neg folders\n",
    "test_files = glob.glob(os.path.join((path/'test/*'),'*.txt'))\n",
    "random.shuffle(test_files)\n",
    "\n",
    "with open('train_data_LM.txt', 'ab') as outfile:\n",
    "    for f in test_files[:int(len(test_files)*0.9)]:\n",
    "        with open(f, 'rb') as infile:\n",
    "            outfile.write(infile.read())\n",
    "with open('test_data_LM.txt',  'ab') as outfile:\n",
    "    for f in test_files[int(len(test_files)*0.9):]:\n",
    "        with open(f, 'rb') as infile:\n",
    "            outfile.write(infile.read())\n",
    "            \n",
    "            \n",
    "# grab all text in the 'unsup' file as well\n",
    "unsup_files = glob.glob(os.path.join((path/'unsup'),'*.txt'))\n",
    "random.shuffle(unsup_files)\n",
    "\n",
    "with open('train_data_LM.txt', 'ab') as outfile:\n",
    "    for f in unsup_files[:int(len(unsup_files)*0.9)]:\n",
    "        with open(f, 'rb') as infile:\n",
    "            outfile.write(infile.read())\n",
    "with open('test_data_LM.txt',  'ab') as outfile:\n",
    "    for f in unsup_files[int(len(unsup_files)*0.9):]:\n",
    "        with open(f, 'rb') as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000, 50000)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_files),len(test_files),len(unsup_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure files are actually shuffled\n",
    "We should be seeing both `pos` and `neg` in the first 10 files of `train_files`. Same goes for `test_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/projectx/.fastai/data/imdb/train/neg/9134_4.txt',\n",
       " '/home/projectx/.fastai/data/imdb/train/neg/1173_1.txt',\n",
       " '/home/projectx/.fastai/data/imdb/train/neg/12174_4.txt',\n",
       " '/home/projectx/.fastai/data/imdb/train/pos/8684_10.txt',\n",
       " '/home/projectx/.fastai/data/imdb/train/neg/5122_3.txt',\n",
       " '/home/projectx/.fastai/data/imdb/train/neg/2807_3.txt',\n",
       " '/home/projectx/.fastai/data/imdb/train/pos/3141_10.txt',\n",
       " '/home/projectx/.fastai/data/imdb/train/neg/7203_2.txt',\n",
       " '/home/projectx/.fastai/data/imdb/train/pos/12078_8.txt',\n",
       " '/home/projectx/.fastai/data/imdb/train/neg/10622_3.txt']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/projectx/.fastai/data/imdb/test/neg/3208_4.txt',\n",
       " '/home/projectx/.fastai/data/imdb/test/neg/494_2.txt',\n",
       " '/home/projectx/.fastai/data/imdb/test/pos/8010_9.txt',\n",
       " '/home/projectx/.fastai/data/imdb/test/pos/779_8.txt',\n",
       " '/home/projectx/.fastai/data/imdb/test/neg/6888_3.txt',\n",
       " '/home/projectx/.fastai/data/imdb/test/neg/4767_3.txt',\n",
       " '/home/projectx/.fastai/data/imdb/test/neg/1191_1.txt',\n",
       " '/home/projectx/.fastai/data/imdb/test/neg/6846_1.txt',\n",
       " '/home/projectx/.fastai/data/imdb/test/neg/6715_1.txt',\n",
       " '/home/projectx/.fastai/data/imdb/test/neg/5814_1.txt']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class training_param():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # general parameters\n",
    "        self.train_data_file = 'train_data_LM.txt'   # train file name\n",
    "        self.eval_data_file = 'test_data_LM.txt'     # test  file name\n",
    "        self.model_name_or_path = 'roberta-base'# change this if using other models\n",
    "        self.block_size = transformer_tokenizer.max_len_single_sentence # The training dataset will be truncated in block of this size for training. Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
    "        self.save_total_limit = 20              # total number of checkpoints we allow\n",
    "        self.output_dir = os.path.join(os.getcwd(),'fine_tuning_LM')    # output directory for checkpoints and saves\n",
    "        self.train_batch_size = 4\n",
    "        self.eval_batch_size  = 4\n",
    "        self.num_train_epochs = 2               # no. of epochs\n",
    "        self.logging_steps    = 50\n",
    "        self.save_steps       = 50\n",
    "        \n",
    "        # optimizer parameters\n",
    "        self.learning_rate    = 5e-5\n",
    "        self.weight_decay     = 0.0\n",
    "        self.adam_epsilon     = 1e-8\n",
    "        self.max_grad_norm    = 1.0             # max gradient norm\n",
    "        self.warmup_steps     = 0               # linear warmup over warmup_steps\n",
    "        \n",
    "        # model parameters\n",
    "        self.fp16             = False\n",
    "        self.fp16_opt_level   = 'O1'            # For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\n",
    "        self.mlm              = True            # Training with masked-language modeling loss instead of vanilla language modeling\n",
    "        self.mlm_probability  = 0.15            # Ratio of tokens to mask for masked language modeling loss\n",
    "        self.device           = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # flags\n",
    "        self.evaluate_during_training = True        \n",
    "        self.overwrite_cache          = True\n",
    "        self.do_train                 = True\n",
    "        self.do_eval                  = True\n",
    "        self.do_lower_case            = True    # True allow us to use uncased model\n",
    "        self.overwrite_output_dir     = True    # overwrite the content of the output directory\n",
    "        self.no_cuda                  = False   # Avoid using CUDA when it's available\n",
    "        self.eval_all_checkpoints     = True    # Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RoBERTa_HP = training_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RoBERTa_HP.num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, HP, file_path=\"fine_tuning_LM\", block_size=510):\n",
    "        assert os.path.isfile(file_path)\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, HP.model_name_or_path + \"_cached_lm_\" + str(block_size) + \"_\" + filename\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not HP.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            with open(file_path, encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "            for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
    "                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size]))\n",
    "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
    "            # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "            # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item])\n",
    "\n",
    "\n",
    "def load_and_cache_examples(HP, tokenizer, evaluate=False):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer,\n",
    "        HP,\n",
    "        file_path=HP.eval_data_file if evaluate else HP.train_data_file,\n",
    "        block_size=HP.block_size,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rotate_checkpoints(HP, checkpoint_prefix, use_mtime=False):\n",
    "    if not HP.save_total_limit:\n",
    "        return\n",
    "    if HP.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    glob_checkpoints = glob.glob(os.path.join(HP.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "    if len(glob_checkpoints) <= HP.save_total_limit:\n",
    "        return\n",
    "\n",
    "    ordering_and_checkpoint_path = []\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - HP.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to HP.save_total_limit\".format(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(inputs: torch.Tensor, tokenizer: PreTrainedTokenizer, HP) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
    "    labels = inputs.clone()\n",
    "    # We sample a few tokens in each sequence for masked-LM training (with probability HP.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "    probability_matrix = torch.full(labels.shape, HP.mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(HP, train_dataset, model, tokenizer=transformer_tokenizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    tb_writer = SummaryWriter()\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=HP.train_batch_size)\n",
    "\n",
    "    t_total = len(train_dataloader)* HP.num_train_epochs\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": HP.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=HP.learning_rate, eps=HP.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=HP.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if os.path.isfile(os.path.join(HP.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
    "        os.path.join(HP.model_name_or_path, \"scheduler.pt\")\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(HP.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(HP.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if HP.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=HP.fp16_opt_level)\n",
    "\n",
    "    \n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", HP.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", HP.train_batch_size)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if os.path.exists(HP.model_name_or_path):\n",
    "        # set global_step to gobal_step of last saved checkpoint from model path\n",
    "        global_step = int(HP.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
    "        epochs_trained = global_step // len(train_dataloader)\n",
    "        steps_trained_in_current_epoch = global_step % (len(train_dataloader))\n",
    "\n",
    "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "        logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "        logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model_to_resize = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model_to_resize.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(HP.num_train_epochs), desc=\"Epoch\", disable=False\n",
    "    )\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = mask_tokens(batch, tokenizer, HP) if HP.mlm else (batch, batch)\n",
    "            inputs = inputs.to(HP.device)\n",
    "            labels = labels.to(HP.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, masked_lm_labels=labels) if HP.mlm else model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if HP.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            \n",
    "            if HP.fp16:\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), HP.max_grad_norm)\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), HP.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            if HP.logging_steps > 0 and global_step % HP.logging_steps == 0:\n",
    "                # Log metrics\n",
    "                if (\n",
    "                    HP.evaluate_during_training\n",
    "                ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                    results = evaluate(HP, model, tokenizer)\n",
    "                    for key, value in results.items():\n",
    "                        tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / HP.logging_steps, global_step)\n",
    "                logging_loss = tr_loss\n",
    "\n",
    "            if HP.save_steps > 0 and global_step % HP.save_steps == 0:\n",
    "                checkpoint_prefix = \"checkpoint\"\n",
    "                # Save model checkpoint\n",
    "                output_dir = os.path.join(HP.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                model_to_save = (\n",
    "                    model.module if hasattr(model, \"module\") else model\n",
    "                )  # Take care of distributed/parallel training\n",
    "                model_to_save.save_pretrained(output_dir)\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                torch.save(HP, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                _rotate_checkpoints(HP, checkpoint_prefix)\n",
    "\n",
    "                torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "    tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(HP, model, tokenizer, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = HP.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(HP, tokenizer=transformer_tokenizer, evaluate=True)\n",
    "\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=HP.eval_batch_size)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", HP.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = mask_tokens(batch, tokenizer, HP) if HP.mlm else (batch, batch)\n",
    "        inputs = inputs.to(HP.device)\n",
    "        labels = labels.to(HP.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, masked_lm_labels=labels) if HP.mlm else model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07/2020 18:19:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "if RoBERTa_HP.eval_data_file is None and RoBERTa_HP.do_eval:\n",
    "        raise ValueError(\n",
    "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
    "            \"or remove the --do_eval argument.\"\n",
    "        )\n",
    "\n",
    "if (\n",
    "    os.path.exists(RoBERTa_HP.output_dir)\n",
    "    and os.listdir(RoBERTa_HP.output_dir)\n",
    "    and RoBERTa_HP.do_train\n",
    "    and not RoBERTa_HP.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            RoBERTa_HP.output_dir\n",
    "        )\n",
    "    )\n",
    "\n",
    "# set up logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO ,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    -1,\n",
    "    device,\n",
    "    1,\n",
    "    False,\n",
    "    RoBERTa_HP.fp16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07/2020 18:19:32 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/projectx/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.9dad9043216064080cf9dd3711c53c0f11fe2b09313eaa66931057b4bdcaf068\n",
      "01/07/2020 18:19:32 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "01/07/2020 18:19:34 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/projectx/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "01/07/2020 18:19:34 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/projectx/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "_, tokenizer_class, config_class , model_class = MODEL_CLASSES[model_type]\n",
    "\n",
    "config = config_class.from_pretrained(RoBERTa_HP.model_name_or_path,\n",
    "                                      do_lower_case = RoBERTa_HP.do_lower_case\n",
    "                                     )\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(RoBERTa_HP.model_name_or_path,\n",
    "                                            do_lower_case = RoBERTa_HP.do_lower_case\n",
    "                                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07/2020 18:20:17 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /home/projectx/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n"
     ]
    }
   ],
   "source": [
    "model = model_class.from_pretrained(\n",
    "        RoBERTa_HP.model_name_or_path,\n",
    "        config  = config\n",
    ")\n",
    "model.to(RoBERTa_HP.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training Begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07/2020 18:22:23 - INFO - __main__ -   Creating features from dataset file at \n",
      "01/07/2020 18:25:34 - INFO - __main__ -   Saving features into cached file roberta-base_cached_lm_510_train_data_LM.txt\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_and_cache_examples(RoBERTa_HP, tokenizer, evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07/2020 18:25:36 - INFO - __main__ -   Creating features from dataset file at \n",
      "01/07/2020 18:25:56 - INFO - __main__ -   Saving features into cached file roberta-base_cached_lm_510_test_data_LM.txt\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_and_cache_examples(RoBERTa_HP, tokenizer, evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2020 12:28:02 - INFO - __main__ -   ***** Running training *****\n",
      "01/06/2020 12:28:02 - INFO - __main__ -     Num examples = 53647\n",
      "01/06/2020 12:28:02 - INFO - __main__ -     Num Epochs = 2\n",
      "01/06/2020 12:28:02 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "01/06/2020 12:28:02 - INFO - __main__ -     Total optimization steps = 26824\n",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/13412 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f65252f30abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mRoBERTa_HP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRoBERTa_HP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-9353f8d29f29>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(HP, train_dataset, model, tokenizer)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "# # The actual training/eval loops\n",
    "# if RoBERTa_HP.do_train:\n",
    "    \n",
    "#     global_step, tr_loss = train(RoBERTa_HP, train_dataset, model, tokenizer)\n",
    "#     logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "    \n",
    "#     # Create output directory if needed\n",
    "#     if not os.path.exists(RoBERTa_HP.output_dir):\n",
    "#         os.makedirs(RoBERTa_HP.output_dir)\n",
    "        \n",
    "#     logger.info(\"Saving model checkpoint to %s\", RoBERTa_HP.output_dir)\n",
    "    \n",
    "#     # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "#     # They can then be reloaded using `from_pretrained()`\n",
    "#     model_to_save = (\n",
    "#         model.module if hasattr(model, \"module\") else model\n",
    "#     )  # Take care of distributed/parallel training\n",
    "#     model_to_save.save_pretrained(RoBERTa_HP.output_dir)\n",
    "#     tokenizer.save_pretrained(RoBERTa_HP.output_dir)\n",
    "\n",
    "#     # Good practice: save your training arguments together with the trained model\n",
    "#     torch.save(RoBERTa_HP, os.path.join(RoBERTa_HP.output_dir, \"training_args.bin\"))\n",
    "\n",
    "#     # Load a trained model and vocabulary that you have fine-tuned\n",
    "#     model = model_class.from_pretrained(RoBERTa_HP.output_dir)\n",
    "#     tokenizer = tokenizer_class.from_pretrained(RoBERTa_HP.output_dir, do_lower_case=RoBERTa_HP.do_lower_case)\n",
    "#     model.to(RoBERTa_HP.device)\n",
    "\n",
    "# results = {}\n",
    "# if RoBERTa_HP.do_eval:\n",
    "#     checkpoints = [RoBERTa_HP.output_dir]\n",
    "#     if RoBERTa_HP.eval_all_checkpoints:\n",
    "#         checkpoints = list(\n",
    "#             os.path.dirname(c) for c in sorted(glob.glob(RoBERTa_HP.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "#         )\n",
    "#         logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "    \n",
    "#     logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "#     for checkpoint in checkpoints:\n",
    "#         global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "#         prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "#         model = model_class.from_pretrained(checkpoint)\n",
    "#         model.to(RoBERTa_HP.device)\n",
    "#         result = evaluate(RoBERTa_HP, model, tokenizer, prefix=prefix)\n",
    "#         result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "#         results.update(result)\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53647, 5947)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/08/2020 09:19:16 - INFO - __main__ -   ***** Running training *****\n",
      "01/08/2020 09:19:16 - INFO - __main__ -     Num examples = 53647\n",
      "01/08/2020 09:19:16 - INFO - __main__ -     Num Epochs = 2\n",
      "01/08/2020 09:19:16 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "01/08/2020 09:19:16 - INFO - __main__ -     Total optimization steps = 26824\n",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch:   0%|          | 0/13412 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=RoBERTa_HP.train_batch_size)\n",
    "t_total = len(train_dataloader)*RoBERTa_HP.num_train_epochs\n",
    "\n",
    "# prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": RoBERTa_HP.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=RoBERTa_HP.learning_rate, eps=RoBERTa_HP.adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=RoBERTa_HP.warmup_steps, \n",
    "                                            num_training_steps=t_total)\n",
    "\n",
    "# Check if saved optimizer or schedular states exist\n",
    "# If do, load them instead\n",
    "if os.path.isfile(os.path.join(RoBERTa_HP.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
    "        os.path.join(RoBERTa_HP.model_name_or_path, \"scheduler.pt\")\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(RoBERTa_HP.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(RoBERTa_HP.model_name_or_path, \"scheduler.pt\")))\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "logger.info(\"  Num Epochs = %d\", RoBERTa_HP.num_train_epochs)\n",
    "logger.info(\"  Instantaneous batch size per GPU = %d\", RoBERTa_HP.train_batch_size)\n",
    "logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "global_step = 0\n",
    "epochs_trained = 0\n",
    "steps_trained_in_current_epoch = 0\n",
    "\n",
    "if os.path.exists(RoBERTa_HP.model_name_or_path):\n",
    "    # set global_step to gobal_step of last saved checkpoint from model path\n",
    "    global_step = int(RoBERTa_HP.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
    "    epochs_trained = global_step // len(train_dataloader)\n",
    "    steps_trained_in_current_epoch = global_step % (len(train_dataloader))\n",
    "\n",
    "    logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "    logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "    logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "    logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "    \n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "model_to_resize = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "model_to_resize.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "train_iterator = trange(\n",
    "        epochs_trained, int(RoBERTa_HP.num_train_epochs), desc=\"Epoch\", disable=False\n",
    "    )\n",
    "\n",
    "epoch_iterator = tqdm(train_dataloader, desc='Epoch', disable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(epoch_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = mask_tokens(batch, tokenizer, RoBERTa_HP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512]), torch.Size([4, 512]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 50264, 50264,  ..., 26991,  4048,     2],\n",
       "        [    0,   822,    21,  ...,   393,    62,     2],\n",
       "        [    0,  1368, 50264,  ...,    45,  2758,     2],\n",
       "        [    0, 48709,  2527,  ..., 50264,  1461,     2]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,  172, 3026,  ..., -100, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100, 9718,  ..., -100, -100, -100],\n",
       "        [-100, -100, -100,  ...,   98, -100, -100]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to(RoBERTa_HP.device)\n",
    "labels = labels.to(RoBERTa_HP.device)\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512]), torch.Size([4, 512]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs, masked_lm_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.roberta(inputs) # sequence output, pooler output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512, 768]), torch.Size([4, 768]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape, outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_output = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 50265])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores = model.lm_head(sequence_output)\n",
    "prediction_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 50265])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores.view(-1,model.config.vocab_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:97",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-6d1abe3cb244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:97"
     ]
    }
   ],
   "source": [
    "masked_lm_loss = loss_fct(prediction_scores.view(-1, model.config.vocab_size), labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8387, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:97",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-d976718bf319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, masked_lm_labels)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmasked_lm_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:97"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs, masked_lm_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, prediction_scores = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-c4886731ec4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "loss.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), torch.Size([4, 512, 50265]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape, prediction_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreakable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0;31m# Special case for 1-item tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    400\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# characters to replace unicode characters with.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mnonzero_finite_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonzero_finite_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36misfinite\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the databunch we created ealier\n",
    "# bs = 48\n",
    "# data_lm = load_data(path, 'lm_databunch_RoBERTa', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] token : <s>\n",
      "[SEP] token : </s>\n",
      "[PAD] token : <pad>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Ġfilm Ġwas Ġone Ġof Ġthe Ġmost Ġoverlooked Ġgems Ġof Ġlast Ġyear . ĠI Ġam Ġdisappointed Ġthat Ġso Ġfew Ġpeople Ġseemed Ġto Ġhave Ġenjoyed Ġthe Ġvery Ġ\" human - ness \" Ġthat Ġthis Ġmovie Ġpresented Ġthe Ġviewer Ġwith .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; I Ġhave Ġread Ġmany Ġbad Ġreviews Ġof Ġthis Ġfilm , Ġand Ġmust Ġadmit Ġa Ġcertain Ġlevel Ġof Ġshock Ġat Ġthe Ġcynicism Ġthat Ġis Ġprevalent Ġin Ġthem .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>! ĠRight Ġfrom Ġthe Ġget - go Ġyou 're Ġlaughing , Ġand Ġbelieve Ġyou Ġme , Ġdon 't Ġplan Ġon Ġresting Ġthat Ġsmile Ġof Ġyours . ĠI Ġpersonally Ġthink Ġthe Ġmovie Ġdefinitely Ġhas Ġbetter Ġmoments Ġthan Ġthe Ġfirst . ĠYou Ġknow Ġwhen Ġyou Ġgo Ġinto Ġa Ġtheater , Ġand Ġyou Ġkind Ġof Ġdon 't Ġwant Ġto Ġhave Ġhigh Ġexpectations Ġfor Ġit ..... well , Ġthis Ġmovie Ġblows Ġall Ġexpectations Ġaway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>... or Ġmaybe Ġyou Ġnotice Ġit Ġhours Ġor Ġdays Ġafter Ġthe Ġfilm Ġends , Ġis Ġthat Ġyou Ġnever Ġsaw Ġany Ġsubstantial Ġplot , Ġyet Ġthe Ġthemes Ġand Ġthe Ġpoetry Ġof Ġthe Ġdialog Ġand Ġcharacters Ġnever Ġleave Ġyou . ĠIn Ġfact , Ġthe Ġtreatment Ġof Ġthe Ġrole Ġrace Ġplays Ġin Ġthe Ġeveryday Ġlives Ġof Ġthese Ġcharacters Ġis Ġalways Ġthere , Ġbut Ġit 's Ġso Ġep hemer al Ġthat Ġeven Ġthey Ġaren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>br Ġ/&gt; In Ġthe Ġbeginning , Ġthose Ġmassive Ġimages Ġcut Ġscenes Ġwere Ġreally Ġpainful Ġto Ġeyes .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; Although Ġit Ġwas Ġnot Ġshown Ġwhen ĠNorman ĠReed us Ġchanged Ġcurrency Ġnotes ? ĠOr Ġis Ġit Ġme Ġstanding Ġby Ġfridge . Ġ:) &lt;/s&gt; &lt;s&gt; ĠJoseph ĠConrad 's Ġtimeless Ġnovel , ĠHeart Ġof ĠDarkness , Ġwas Ġdepicted Ġin Ġthe Ġ1994 Ġmovie . ĠI Ġhave Ġread ĠConrad 's Ġnovel ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Ġonly Ġblack Ġmale Ġon Ġthe Ġset ĠJohn ĠT oles - Bey Ġwho Ġmust Ġlook Ġat Ġthis Ġmovie Ġand Ġjust Ġwonder . ĠLook Ġhim Ġup Ġsometime Ġas Ġhe Ġhas Ġdone Ġa Ġlot Ġof Ġinteresting Ġmovies .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; But Ġon Ġthis Ġmovie : ĠThe Ġscript Ġas ĠI Ġsaid Ġearlier Ġwas Ġa Ġrip - off Ġof ĠAliens Ġtweaked Ġand Ġturned Ġinto Ġa Ġsubmarine Ġ\" th r iller \". ĠIt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a sanity check for <s>; </s>; <pad>\n",
    "print('[CLS] token :', transformer_tokenizer.cls_token)\n",
    "print('[SEP] token :', transformer_tokenizer.sep_token)\n",
    "print('[PAD] token :', transformer_tokenizer.pad_token)\n",
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We would be feeding for each batch size of `32` and bptt of `70` (This is by defaul of FastAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] id : 0\n",
      "[SEP] id : 2\n",
      "[PAD] id : 1\n",
      "Batch shape :  torch.Size([32, 70])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[  287,    10, 12003,  ...,   172,   939,  2037],\n",
       "         [    4,   318,    47,  ...,    14,    47,    58],\n",
       "         [   75,  2542,     9,  ...,  2610,  5270,    36],\n",
       "         ...,\n",
       "         [ 1589, 49007,  3809,  ...,    42,     6,   213],\n",
       "         [   16,  2679, 49069,  ...,    39, 37390,   455],\n",
       "         [    2,     0,   272,  ...,     8,  6906,     9]]),\n",
       " tensor([[   10, 12003,  1294,  ...,   939,  2037,    41],\n",
       "         [  318,    47,   657,  ...,    47,    58,  4441],\n",
       "         [ 2542,     9,   141,  ...,  5270,    36,    90],\n",
       "         ...,\n",
       "         [49007,  3809, 48709,  ...,     6,   213,  1183],\n",
       "         [ 2679, 49069,  3809,  ..., 37390,   455,     9],\n",
       "         [    0,   272,  8508,  ...,  6906,     9,  1133]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('[CLS] id :', transformer_tokenizer.cls_token_id)\n",
    "print('[SEP] id :', transformer_tokenizer.sep_token_id)\n",
    "print('[PAD] id :', pad_idx)\n",
    "test_one_batch = data_lm.one_batch()\n",
    "print('Batch shape : ',test_one_batch[0].shape)\n",
    "test_one_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize model\n",
    "As mentioned [here](https://github.com/huggingface/transformers#models-always-output-tuples), every model's forward method always outputs a tuple with various elements depending on the model and the configuration parameters. In our case, we are interested to access only to the logits.  One way to access them is to create a custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.roberta??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our model architecture \n",
    "class CustomTransformerModel_Encoder(nn.Module):\n",
    "    def __init__(self, transformer_model: PreTrainedModel):\n",
    "        super(CustomTransformerModel_Encoder,self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \n",
    "        #attention_mask = (input_ids!=1).type(input_ids.type()) # Test attention_mask for RoBERTa\n",
    "        \n",
    "        logits = self.transformer(input_ids, attention_mask = attention_mask)\n",
    "        \n",
    "        return logits\n",
    "    # this function is added because fastai `learner.lr_find()` will call it\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pretrained weights for RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 50265,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is the configuration of language model\n",
    "config = config_class.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = transformer_tokenizer.vocab_size\n",
    "config.output_hidden_states = True\n",
    "config.output_attentions = True\n",
    "config.use_bfloat16 = use_fp16\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model_LM = LM_model.from_pretrained(pretrained_model_name, config = config)\n",
    "\n",
    "custom_transformer_model_LM = CustomTransformerModel_Encoder(transformer_model = transformer_model_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import *\n",
    "from transformers import AdamW\n",
    "from functools import partial\n",
    "\n",
    "CustomAdamW = partial(AdamW, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# custom_transformer_model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split layers into layer groups\n",
    "def split(model, split_on:SplitFuncOrIdxList)->None:\n",
    "    \"Split the model at `split_on`.\"\n",
    "    if isinstance(split_on,Callable): split_on = split_on(self.model)\n",
    "    layer_groups = split_model(model, split_on)\n",
    "    return layer_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768, padding_idx=1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enc = custom_transformer_model.transformer.embeddings.word_embeddings\n",
    "# enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our decoder will share the weights of the embedding layer\n",
    "This is to convert the 768 feature dimension back to probability of our 50265 vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctx_len': 512,\n",
       " 'n_layers': 12,\n",
       " 'n_heads': 12,\n",
       " 'd_model': 768,\n",
       " 'd_head': 64,\n",
       " 'd_inner': 3072,\n",
       " 'resid_p': 0.1,\n",
       " 'attn_p': 0.1,\n",
       " 'ff_p': 0.1,\n",
       " 'embed_p': 0.1,\n",
       " 'output_p': 0.0,\n",
       " 'bias': True,\n",
       " 'scale': True,\n",
       " 'act': <Activation.GeLU: 3>,\n",
       " 'double_drop': False,\n",
       " 'tie_weights': True,\n",
       " 'out_bias': False,\n",
       " 'init': <function fastai.text.models.transformer.init_transformer(m)>,\n",
       " 'mask': True}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out whether or not we should set `tie_weights` to be True for transformers in FastAI\n",
    "# `n_hid` = 768\n",
    "# Also `bias` need to be True\n",
    "# `output_p` set to 0.0\n",
    "tfmer_lm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-construct decoder using FastAI LinearDecoder\n",
    "\n",
    "decoder = LinearDecoder(transformer_tokenizer.vocab_size, \n",
    "                        n_hid=768, \n",
    "                        output_p=0.0,\n",
    "                        tie_encoder=enc,\n",
    "                        bias=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDecoder(\n",
       "  (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  (output_dp): RNNDropout()\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialRNN(custom_transformer_model.transformer, decoder)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_fastai = get_language_model(Transformer, vocab_sz=50265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    7, 16679,  7768,  ...,    21,   372,    21],\n",
       "         [    5,  1455,   183,  ...,    65,     9, 28505],\n",
       "         [ 1235, 49069,  3809,  ...,    15,     5, 15143],\n",
       "         ...,\n",
       "         [    9,     5,   367,  ...,   252,  5956,    41],\n",
       "         [   16,   988,     6,  ...,   101,     8,   172],\n",
       "         [17743,    43,     7,  ...,     4,  7538,     5]]),\n",
       " tensor([[16679,  7768,     8,  ...,   372,    21,    14],\n",
       "         [ 1455,   183,     4,  ...,     9, 28505, 49069],\n",
       "         [49069,  3809,  1589,  ...,     5, 15143, 45677],\n",
       "         ...,\n",
       "         [    5,   367,  4133,  ...,  5956,    41,  3537],\n",
       "         [  988,     6,   102,  ...,     8,   172,    88],\n",
       "         [   43,     7,   492,  ...,  7538,     5,   169]]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = data_lm.one_batch()\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 70]), torch.Size([32, 70]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(), y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 70, 50265]),\n",
       " torch.Size([32, 70, 768]),\n",
       " torch.Size([32, 70, 768]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].size(), result[1][0].size(), result[2][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = transformer_fastai(x)\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the ourput list is 3\n"
     ]
    }
   ],
   "source": [
    "result_2 = custom_transformer_model_LM(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 70, 50265])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 12, 70, 70])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2[2][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaLMHead(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (layer_norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "  (decoder): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_transformer_model_LM.transformer.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoBERTa_lm_split(model:nn.Module) -> List[nn.Module]:\n",
    "    \"Split a RoBERTa `model` in groups for differential learning rates.\"\n",
    "    encoder = model.transformer.roberta.encoder\n",
    "    n = len(encoder.layer)//3\n",
    "    groups = [list(encoder.layer[:n]), list(encoder.layer[n:2*n]), list(encoder.layer[2*n:])]\n",
    "    \n",
    "    return groups + [[model.transformer.roberta.embeddings.word_embeddings, model.transformer.lm_head]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfmer_lm_split??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm = LanguageLearner(data_lm, \n",
    "                           custom_transformer_model_LM,\n",
    "                           split_func=RoBERTa_lm_split,\n",
    "                           opt_func = CustomAdamW)\n",
    "learn_lm.callbacks.append(ShowGraph(learn_lm))\n",
    "\n",
    "# Put learn in FP16 precision mode. --> Seems to not working\n",
    "# if use_fp16: learn_lm = learn_lm.to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer grouping\n",
    "\n",
    "FastAI uses layer grouping for primarily Three objectives:\n",
    "1. discrimilative learning rate\n",
    "2. gradual unfreezing\n",
    "3. saving `state_dict` for certain parts of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_lm.model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For roberta-language-model\n",
    "\n",
    "list_layers = [\n",
    "              [learn_lm.model.transformer.roberta.embeddings,\n",
    "               learn_lm.model.transformer.roberta.encoder.layer[0],\n",
    "               learn_lm.model.transformer.roberta.encoder.layer[1],\n",
    "               learn_lm.model.transformer.roberta.encoder.layer[2]],\n",
    "              [learn_lm.model.transformer.roberta.encoder.layer[3],\n",
    "               learn_lm.model.transformer.roberta.encoder.layer[4],\n",
    "               learn_lm.model.transformer.roberta.encoder.layer[5]],\n",
    "              [learn_lm.model.transformer.roberta.encoder.layer[6],\n",
    "               learn_lm.model.transformer.roberta.encoder.layer[7],\n",
    "               learn_lm.model.transformer.roberta.encoder.layer[8]],\n",
    "              [learn_lm.model.transformer.roberta.encoder.layer[9],\n",
    "               learn_lm.model.transformer.roberta.encoder.layer[10]],\n",
    "              [learn_lm.model.transformer.roberta.encoder.layer[11],\n",
    "               learn_lm.model.transformer.roberta.pooler],\n",
    "              [\n",
    "               learn_lm.model.transformer.lm_head]\n",
    "              ]\n",
    "\n",
    "# list_layers = [learn_lm.model[0],\n",
    "#               learn_lm.model[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner split in 4 groups\n",
      "[Sequential(\n",
      "  (0): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (1): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (2): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (3): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (1): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (2): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (3): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (1): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (2): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (3): BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): Embedding(50265, 768, padding_idx=1)\n",
      "  (1): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=False)\n",
      "  )\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "# learn_lm.split(list_layers)\n",
    "num_groups = len(learn_lm.layer_groups)\n",
    "print('Learner split in',num_groups,'groups')\n",
    "print(learn_lm.layer_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the model is split into two part Encoder and classifier which enables us to save just the encoder weights later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  190,   114,    47,  ...,   817,  1472,   142],\n",
       "         [  133,    86,  3974,  ...,  3809, 48709, 14699],\n",
       "         [  884,     4,    38,  ...,     6,    14,    38],\n",
       "         ...,\n",
       "         [   19,   299, 16046,  ...,     4,    38, 16112],\n",
       "         [    5,  1569,     4,  ...,  2280,   173,   259],\n",
       "         [ 2439,  1416,    23,  ...,     0,    38,    33]]),\n",
       " tensor([[  114,    47,   218,  ...,  1472,   142,    51],\n",
       "         [   86,  3974,     8,  ..., 48709, 14699,  2972],\n",
       "         [    4,    38,   109,  ...,    14,    38,   465],\n",
       "         ...,\n",
       "         [  299, 16046,  2038,  ...,    38, 16112,   492],\n",
       "         [ 1569,     4,   272,  ...,   173,   259,     6],\n",
       "         [ 1416,    23,     5,  ...,    38,    33,   450]]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = data_lm.one_batch()\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the model and turn it to mixed-precision mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 8.32E-04\n",
      "Min loss divided by 10: 1.10E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_lm.freeze()\n",
    "\n",
    "learn_lm.lr_find()\n",
    "learn_lm.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.271727</td>\n",
       "      <td>0.202913</td>\n",
       "      <td>0.971420</td>\n",
       "      <td>48:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.241903</td>\n",
       "      <td>0.195251</td>\n",
       "      <td>0.972543</td>\n",
       "      <td>47:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.213293</td>\n",
       "      <td>0.180257</td>\n",
       "      <td>0.974419</td>\n",
       "      <td>46:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.190232</td>\n",
       "      <td>0.172095</td>\n",
       "      <td>0.975097</td>\n",
       "      <td>46:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.191394</td>\n",
       "      <td>0.170125</td>\n",
       "      <td>0.975388</td>\n",
       "      <td>46:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 8.32E-04\n",
    "learn_lm.fit_one_cycle(5, lr, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save('language_model_IMDb_freezed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 6.31E-07\n",
      "Min loss divided by 10: 1.91E-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_lm.unfreeze()\n",
    "\n",
    "learn_lm.lr_find()\n",
    "learn_lm.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.110532</td>\n",
       "      <td>0.086335</td>\n",
       "      <td>0.987574</td>\n",
       "      <td>1:01:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.100117</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>0.987680</td>\n",
       "      <td>1:01:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.102922</td>\n",
       "      <td>0.084077</td>\n",
       "      <td>0.987677</td>\n",
       "      <td>1:01:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-6\n",
    "learn_lm.fit_one_cycle(3, lr, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New our language model is more IMDb-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model(learn_lm.model)[0].transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the encoder of the language model and save it's encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_model(learn_lm.model)[0].transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'learn_lm_encoder_IMDB'\n",
    "torch.save(encoder.state_dict(), learn_lm.path/learn_lm.model_dir/f'{name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/projectx/.fastai/data/imdb/models/learn_clas_unfreed_second.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/no-pre-training-freezed.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/fit_freezed_5_epochs.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/fine_tuned_lm_enc.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/tmp.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/language_model_IMDb_freezed.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/learn_clas_unfreezed_third.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/learn_clas_unfreezed_final.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/learn_lm_encoder_IMDB.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/untrained_classifier.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/no-pre-training-freez_to_-5.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/fine_tuned_lm.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/learn_clas_freezed.pth'),\n",
       " PosixPath('/home/projectx/.fastai/data/imdb/models/no-pre-training-freez_to_-3.pth')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'models').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataBunch for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can play around with the batch_size as long as the GPU can take it\n",
    "bs = 16\n",
    "data_clas = (TextList.from_folder(path, processor=transformer_processor) # specify the path\n",
    "           .filter_by_folder(include=['train','test'])    # exclude other folders\n",
    "           .split_by_folder(valid='test')                 # split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "           .label_from_folder(classes=['neg', 'pos'])     # label them all with their folders\n",
    "           .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))                             # convert to databunch for the learner later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] token : <s>\n",
      "[SEP] token : </s>\n",
      "[PAD] token : <pad>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>&lt;s&gt; ĠIt 's Ġobvious Ġthat Ġthe Ġpeople Ġwho Ġmade Ġ' Dead ĠAt ĠThe ĠBox ĠOffice ' Ġlove ĠB - movie Ġhorror . ĠOvert Ġreferences Ġto Ġthe Ġgenre Ġare Ġpepp ered Ġthroughout , Ġfrom Ġstock Ġcharacters Ġ( the Ġauthority Ġfigure Ġwho Ġdoesn 't Ġbelieve Ġthe Ġmonstrous Ġinvasion Ġis Ġreally Ġhappening ) Ġto ĠKevin ĠSmith Ġstyle Ġdiscussions Ġto Ġre en acting ĠDu ane ĠJones ' Ġlast Ġmoments Ġfrom Ġ' Night Ġof Ġthe</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;s&gt; ĠThe Ġfirst Ġpart , ĠChe Ġin ĠCuba , Ġis Ġabout Ġthat Ġportion Ġof Ġhis Ġlife . ĠIt Ġcontains Ġtoo Ġmany Ġindistinguishable Ġbattles Ġand ĠChe Ġminister ing Ġto Ġtoo Ġmany Ġindistinguishable Ġwounded Ġ( remember Ġthat ĠChe Ġwas Ġa Ġphysician ). ĠIt Ġends Ġas ĠCastro Ġwins Ġthe Ġrevolution ; ĠChe Ġnever Ġgets Ġto ĠHavana . ĠThe Ġsecond Ġpart , ĠChe Ġin ĠBolivia , Ġis Ġabout Ġguess Ġwhat . ĠIt Ġcontains Ġtoo</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;s&gt; Ġ\" Mem oir s Ġof Ġa ĠGe isha \" Ġis Ġa Ġvisually Ġstunning Ġmel od rama Ġthat Ġseems Ġmore Ġlike Ġa Ġcamp , Ġdrag Ġqueen Ġsatire Ġthan Ġanything Ġto Ġdo Ġwith Ġreal Ġpeople .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; The Ġfirst Ġhalf Ġof Ġthe Ġfilm Ġdefensively Ġkeeps Ġinsisting Ġthat Ġge ish as Ġare Ġneither Ġprostitutes Ġnor Ġconc ub ines , Ġthat Ġthey Ġare Ġthe Ġembodiment Ġof Ġtraditional ĠJapanese Ġbeauty</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;s&gt; ĠWow . ĠWatching Ġthis Ġfilm Ġtoday , Ġyou Ġcan 't Ġhelp Ġbut Ġbe Ġappalled Ġby Ġthe Ġwriting Ġof Ġthis Ġfilm . ĠSpencer ĠTracy Ġand ĠLore tta ĠYoung Ġplay Ġa Ġcouple Ġwho , Ġin Ġmodern Ġtimes , Ġmight Ġbe Ġfeatured Ġon Ġ\" The ĠJerry ĠSpringer ĠShow \" -- as Ġthey Ġhave Ġa Ġsick Ġand Ġabusive Ġrelationship ... and Ġinexpl icably , Ġthe Ġwriters Ġappear Ġto Ġbe Ġendorsing Ġit ! Ġ&lt;</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;s&gt; ĠI Ġhaven 't Ġseen Ġall Ġof ĠJess ĠFranco 's Ġmovies , ĠI Ġhave Ġseen Ġ5 , ĠI Ġthink , Ġand Ġthere Ġare Ġmore Ġthan Ġ180 Ġof Ġthem . ĠSo Ġmaybe Ġit 's Ġa Ġbit Ġearly Ġto Ġsay Ġso Ġbut Ġ\" N ec ron om icon ĠGet r Ã¤ um te ĠS Ã¼ nd en \" Ġ( better Ġknown Ġas Ġ' Su cc ub us ', Ġbut Ġthat Ġis Ġthe</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('[CLS] token :', transformer_tokenizer.cls_token)\n",
    "print('[SEP] token :', transformer_tokenizer.sep_token)\n",
    "print('[PAD] token :', transformer_tokenizer.pad_token)\n",
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] id : 0\n",
      "[SEP] id : 2\n",
      "[PAD] id : 1\n",
      "Batch shape :  torch.Size([16, 512])\n",
      "(tensor([[    0,    38,  4157,  ...,    14,   847,     2],\n",
      "        [    0, 44776,  8685,  ...,   113,     8,     2],\n",
      "        [    0,    20, 28609,  ..., 18756,  7560,     2],\n",
      "        ...,\n",
      "        [    0,    96,     5,  ...,   430,     8,     2],\n",
      "        [    0, 31906,    14,  ...,  8545,     6,     2],\n",
      "        [    0,    36, 19933,  ..., 17818,   322,     2]]), tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print('[CLS] id :', transformer_tokenizer.cls_token_id)\n",
    "print('[SEP] id :', transformer_tokenizer.sep_token_id)\n",
    "print('[PAD] id :', pad_idx)\n",
    "test_one_batch = data_clas.one_batch()\n",
    "print('Batch shape : ',test_one_batch[0].shape)\n",
    "print(test_one_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our model architecture \n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, transformer_model: PreTrainedModel):\n",
    "        super(CustomTransformerModel,self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \n",
    "        #attention_mask = (input_ids!=1).type(input_ids.type()) # Test attention_mask for RoBERTa\n",
    "        \n",
    "        logits = self.transformer(input_ids,\n",
    "                                attention_mask = attention_mask)[0]   \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = config_class.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = 2\n",
    "config.use_bfloat16 = use_fp16\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n",
    "# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)\n",
    "\n",
    "custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the encoder's pre-trained weights from the language model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved weights \n",
    "custom_transformer_model.transformer.roberta.load_state_dict(torch.load(learn_lm.path/learn_lm.model_dir/f'{name}.pth', map_location=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import *\n",
    "from transformers import AdamW\n",
    "from functools import partial\n",
    "\n",
    "CustomAdamW = partial(AdamW, correct_bias=False)\n",
    "\n",
    "learner = Learner(data_clas, \n",
    "                  custom_transformer_model, \n",
    "                  opt_func = CustomAdamW, \n",
    "                  metrics=[accuracy, error_rate])\n",
    "\n",
    "# Show graph of learner stats and metrics after each epoch.\n",
    "learner.callbacks.append(ShowGraph(learner))\n",
    "\n",
    "# Put learn in FP16 precision mode. --> Seems to not working\n",
    "if use_fp16: learner = learner.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTransformerModel(\n",
      "  (transformer): RobertaForSequenceClassification(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (classifier): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(learner.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For roberta-base\n",
    "list_layers = [learner.model.transformer.roberta.embeddings,\n",
    "              learner.model.transformer.roberta.encoder.layer[0],\n",
    "              learner.model.transformer.roberta.encoder.layer[1],\n",
    "              learner.model.transformer.roberta.encoder.layer[2],\n",
    "              learner.model.transformer.roberta.encoder.layer[3],\n",
    "              learner.model.transformer.roberta.encoder.layer[4],\n",
    "              learner.model.transformer.roberta.encoder.layer[5],\n",
    "              learner.model.transformer.roberta.encoder.layer[6],\n",
    "              learner.model.transformer.roberta.encoder.layer[7],\n",
    "              learner.model.transformer.roberta.encoder.layer[8],\n",
    "              learner.model.transformer.roberta.encoder.layer[9],\n",
    "              learner.model.transformer.roberta.encoder.layer[10],\n",
    "              learner.model.transformer.roberta.encoder.layer[11],\n",
    "              learner.model.transformer.roberta.pooler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner split in 14 groups\n",
      "[Sequential(\n",
      "  (0): Embedding(50265, 768, padding_idx=1)\n",
      "  (1): Embedding(514, 768, padding_idx=1)\n",
      "  (2): Embedding(1, 768)\n",
      "  (3): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (4): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (6): Dropout(p=0.1)\n",
      "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (9): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  (10): Dropout(p=0.1)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (3): Dropout(p=0.1)\n",
      "  (4): Linear(in_features=768, out_features=2, bias=True)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "learner.split(list_layers)\n",
    "num_groups = len(learner.layer_groups)\n",
    "print('Learner split in',num_groups,'groups')\n",
    "print(learner.layer_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('untrained_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.load('untrained_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learner.freeze()\n",
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min numerical gradient: 6.92E-06\n",
      "Min loss divided by 10: 5.25E-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      60.00% [3/5 29:05<19:23]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.680777</td>\n",
       "      <td>0.683326</td>\n",
       "      <td>0.565560</td>\n",
       "      <td>0.434440</td>\n",
       "      <td>09:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.683535</td>\n",
       "      <td>0.680124</td>\n",
       "      <td>0.565920</td>\n",
       "      <td>0.434080</td>\n",
       "      <td>09:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.682704</td>\n",
       "      <td>0.677661</td>\n",
       "      <td>0.588000</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>10:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='1562', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/projectx/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-68c4c75bcd19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2e-4\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_backward_begin\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;34m\"Handle gradient calculation on `loss`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'smooth_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'backward_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(5, 2e-4 ,moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old ULMFiT code for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Padding_idx must be within num_embeddings",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-e950d9b385f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# the drop_mult decide the percentage of dropout to use in relation to the combination used in the original paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearn_lm_AWD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAWD_LSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/text/learner.py\u001b[0m in \u001b[0;36mlanguage_model_learner\u001b[0;34m(data, arch, config, drop_mult, pretrained, pretrained_fnames, **learn_kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            pretrained_fnames:OptStrTuple=None, **learn_kwargs) -> 'LanguageLearner':\n\u001b[1;32m    203\u001b[0m     \u001b[0;34m\"Create a `Learner` with a language model from `data` and `arch`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_mult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_model_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split_lm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlearn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/text/learner.py\u001b[0m in \u001b[0;36mget_language_model\u001b[0;34m(arch, vocab_sz, config, drop_mult)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mtie_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'tie_weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output_p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'out_bias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'init'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'init'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtie_weights\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hid_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtie_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pre_init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mold_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__post_init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/text/models/awd_lstm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token, hidden_p, input_p, embed_p, weight_p, qrnn, bidir)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memb_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_hid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbidir\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqrnn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpadding_idx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mpadding_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding_idx must be within num_embeddings'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpadding_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mpadding_idx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding_idx must be within num_embeddings'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Padding_idx must be within num_embeddings"
     ]
    }
   ],
   "source": [
    "# the drop_mult decide the percentage of dropout to use in relation to the combination used in the original paper\n",
    "learn_lm_AWD = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "As we can see below, when we create `TextDataBunch` a  `vocab` property is created simultaneously. <br>\n",
    "It's worth pointing out that, for `index-to-strings` and `strings-to-index` we have a different length. This is primarily because that there are some words that appear infrequently so that it's not effcient for them to occupy one token. What we do is to map all of these low frequency words to `xxunk`. We can ditermine the `min_freq` in the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = data_lm.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35055"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi[\"stingray\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi[\"happy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxunk'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.itos[vocab.stoi['mamamia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(learn_lm.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60000, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=60000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ourmodel has two parts, the AWD_LSTM base architecture and the linear classifier\n",
    "learn_lm.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments on `drop_mult`\n",
    "From the original [AWD_LSTM paper](https://arxiv.org/pdf/1708.02182.pdf) the authors applied several effective techniques.<br>\n",
    "\n",
    "**DropConnect** was implemented in the architecture in that weight matrices are *dropped* before the *forward* and *backward* pass.<br>\n",
    "\n",
    "**Variational dropout**. In standard dropout, a new binary dropout mask is sampled each and every time the dropout function is called. **Variational dropout** only samples a *dropout mask* upon the first call and then will repeatedly use that *locked dropout* mask for all repeated connections within the forward and backward pass.<br>\n",
    "\n",
    "The values used for *dropout on the word vectors*, the *output between LSTM layers*, the *output of the final LSTM layer*, and *embedding dropout* were (0.4, 0.3, 0.4, 0.1), respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "Encoder is the first layer of the AWD_LSTM, which is also known as the `embedding layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 400])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = learn_lm.model[0].encoder\n",
    "enc.weight.size()  # 400 is the embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall that 60000 is the vocab size of our language model vocabulary\n",
    "len(data_lm.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before training\n",
    "Let's try to use this model(only pre-trained with wiki-text) to generate fake movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = 'The color of the sky is'\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The color of the sky is often changed from black to blue , to give it a more light color . The color is derived from the colours of one of the sky planets and the blue sky . The bird 's colour is\n",
      "\n",
      "The color of the sky is a reference to the thin red hair of the Earth . The physical presence of the moon in the sky and the dark sky in the dark , and the so - called Earth - like sky\n"
     ]
    }
   ],
   "source": [
    "# Note that the 'temperature' denote the randomness we adopt when pick next words\n",
    "print(\"\\n\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The color of the sky is a matter of controversy , as the Sun is not a white sky , but a white sky . The Sun is a dark , dark , dark , dark , dark , dark , dark ,\n",
      "\n",
      "The color of the sky is a matter of debate , and the International Union of Red and White Stars ( NAACP ) has been the most popular group of the American public . The American\n",
      "\n",
      "The color of the sky is a matter of controversy , as the Sun has been described as a \" dark , dark , dark , dark , dark , dark , dark , dark , dark , dark , dark , dark , dark\n"
     ]
    }
   ],
   "source": [
    "# If 'temperature'  set to super low, there will be almost no randomness\n",
    "N_SENTENCES = 3\n",
    "print(\"\\n\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.05) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate this movie so much it does not last for a single , less weekly , or just a few more pages of a book , so it can be seen as Lily Allen 's second novel The Spirit of\n",
      "\n",
      "I hate this movie so much that i think i would be thinking about it . The greatest reason why i wanted to make a movie about a movie was that you would not let it go , because it was not a chance to\n",
      "\n",
      "I hate this movie so much that i can ' t do that . It could make you want to see what is right . But i don ' t hope to see what the right thing can do , and as many as\n"
     ]
    }
   ],
   "source": [
    "# Change TEXT\n",
    "TEXT = 'I hate this movie so much'\n",
    "print(\"\\n\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc(LanguageLearner.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning a language model\n",
    "We can use the `data_lm` object we created earlier to fine-tune a pretrained language model. \n",
    "Here we will be using a pre-trained `AWD-LSTM` architecture theat is available in FastAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn_lm.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot function attached to lr_finder\n",
    "# Note that we get a smoothened version where `skip_start` and `skip_end` decide howmuch to trim-off from start or end\n",
    "\n",
    "def plot(self, skip_start:int=10, skip_end:int=5, suggestion:bool=False, return_fig:bool=None,\n",
    "             **kwargs)->Optional[plt.Figure]:\n",
    "        \"Plot learning rate and losses, trimmed between `skip_start` and `skip_end`. Optionally plot and return min gradient\"\n",
    "        lrs = self._split_list(self.lrs, skip_start, skip_end)\n",
    "        losses = self._split_list(self.losses, skip_start, skip_end)\n",
    "        losses = [x.item() for x in losses]\n",
    "        if 'k' in kwargs: losses = self.smoothen_by_spline(lrs, losses, **kwargs)\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        ax.plot(lrs, losses)\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Learning Rate\")\n",
    "        ax.set_xscale('log')\n",
    "        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n",
    "        if suggestion:\n",
    "            try: mg = (np.gradient(np.array(losses))).argmin()\n",
    "            except:\n",
    "                print(\"Failed to compute the gradients, there might not be enough points.\")\n",
    "                return\n",
    "            print(f\"Min numerical gradient: {lrs[mg]:.2E}\")\n",
    "            ax.plot(lrs[mg],losses[mg],markersize=10,marker='o',color='red')\n",
    "            self.min_grad_lr = lrs[mg]\n",
    "            ml = np.argmin(losses)\n",
    "            print(f\"Min loss divided by 10: {lrs[ml]/10:.2E}\")\n",
    "        if ifnone(return_fig, defaults.return_fig): return fig\n",
    "        if not IN_NOTEBOOK: plot_sixel(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_lm.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trim the last 15 datapoints so that we have clearer view\n",
    "learn_lm.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to select a `learning_rate` where the slope is steep and not going upwards.<br>\n",
    "`lr=5e-3` seems to be a sensible choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.167812</td>\n",
       "      <td>4.067869</td>\n",
       "      <td>0.291104</td>\n",
       "      <td>21:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.200355</td>\n",
       "      <td>4.070581</td>\n",
       "      <td>0.290157</td>\n",
       "      <td>21:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.133083</td>\n",
       "      <td>4.029402</td>\n",
       "      <td>0.293869</td>\n",
       "      <td>21:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.056716</td>\n",
       "      <td>3.989157</td>\n",
       "      <td>0.298245</td>\n",
       "      <td>21:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.998321</td>\n",
       "      <td>3.979450</td>\n",
       "      <td>0.299676</td>\n",
       "      <td>21:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr=5e-3\n",
    "learn_lm.to_fp16()\n",
    "learn_lm.fit_one_cycle(5, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save('fit_freezed_5_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.load('fit_freezed_5_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.806393</td>\n",
       "      <td>3.795424</td>\n",
       "      <td>0.318980</td>\n",
       "      <td>23:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.786836</td>\n",
       "      <td>3.768348</td>\n",
       "      <td>0.324682</td>\n",
       "      <td>23:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.763445</td>\n",
       "      <td>3.743657</td>\n",
       "      <td>0.328573</td>\n",
       "      <td>23:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.717152</td>\n",
       "      <td>3.711501</td>\n",
       "      <td>0.332588</td>\n",
       "      <td>23:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.619674</td>\n",
       "      <td>3.682857</td>\n",
       "      <td>0.335828</td>\n",
       "      <td>23:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.578209</td>\n",
       "      <td>3.660896</td>\n",
       "      <td>0.338813</td>\n",
       "      <td>23:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.528988</td>\n",
       "      <td>3.642530</td>\n",
       "      <td>0.341307</td>\n",
       "      <td>23:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.461757</td>\n",
       "      <td>3.632708</td>\n",
       "      <td>0.342691</td>\n",
       "      <td>23:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.392899</td>\n",
       "      <td>3.631087</td>\n",
       "      <td>0.343060</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.364762</td>\n",
       "      <td>3.633080</td>\n",
       "      <td>0.342879</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now we see that the aacuracy is positively improving\n",
    "# we can unfreeze and train further\n",
    "learn_lm.unfreeze()\n",
    "learn_lm.fit_one_cycle(10,lr/5, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save('fine_tuned_lm')\n",
    "learn_lm.save_encoder('fine_tuned_lm_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"i liked this movie because\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i liked this movie because it was true to life , a true story about a lame social worker who gets his to check out his own fantasy club . He always thinks he is an artist , but he ca n't see how\n",
      "\n",
      "i liked this movie because of the heroic acts Richard Gere said to Richard Gere . i liked the way the film was supposed to be , and i was surprised how little was done in the moment . It\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was really bad . The acting was poor , and the story had no excitement . And the \" plot \" , was non - existent , and had\n",
      "\n",
      "This movie was clearly done by people who did n't know what the TV series was . The whole story line was n't very interesting . The acting was quite\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"This movie was\"\n",
    "N_WORDS = 30\n",
    "N_SENTENCES = 2\n",
    "print(\"\\n\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the language model is much more \"*IMDB like*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 48\n",
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
    "             #grab all the text files in path\n",
    "             .split_by_folder(valid='test')\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_folder(classes=['neg', 'pos'])\n",
    "             #label them all with their folders\n",
    "             .databunch(bs=bs, num_workers=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.save('imdb_textlist_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj some have praised _ xxunk _ as a xxmaj disney adventure for adults . i do n't think so -- at least not for thinking adults . \\n \\n  xxmaj this script suggests a beginning as a live - action movie , that struck someone as the type of crap you can not sell to adults anymore . xxmaj the \" crack staff \" of many older</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos * * * xxmaj warning - this review contains \" plot spoilers , \" though nothing could \" spoil \" this movie any more than it already is . xxmaj it really xxup is that bad . * * * \\n \\n  xxmaj before i begin , i 'd like to let everyone know that this definitely is one of those so - incredibly - bad - that</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj okay , so i 'm not a big video game buff , but was the game xxmaj house of the xxmaj dead really famous enough to make a movie from ? xxmaj sure , they went as far as to actually put in quick video game clips throughout the movie , as though justifying any particular scene of violence , but there are dozens and dozens of games</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj in xxup nyc , seaman xxmaj michael o'hara ( xxmaj orson xxmaj welles ) rescues xxmaj elsa xxmaj bannister ( xxmaj rita xxmaj hayworth ) from a mugging &amp; rape as she takes a horse &amp; carriage through xxmaj central xxmaj park -and lives to regret it . xxmaj xxunk - haired xxmaj hayworth 's a platinum blonde in this one ; as dazzling as fresh - fallen</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating ultimate-model: IMDB review classifier\n",
    "Three simple steps to create our review classifier after fine tuning our language model with IMDb dataset:\n",
    "    1. Create a `text_classifier_learner` with `AWD_LSTM` as base archietecture(note that we can use any model here as base architecture, but it has to be the same with the one we just trained as language model. So if we want a transformer here, we would be training a transformer language model in the previous section)\n",
    "    2. Load the `fine-tuned-language-model-encoder`. We have save both the encoder as well as the entire model. We would only be needing the encoder's weights here as we will be replacing the head as a freshly initialize classifier and train it later.\n",
    "    3. Freeze the body(base architecture) of our model, and train just the head first. We can make use of our usual trategy be looking for the optimal learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_clas = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3)\n",
    "learn_clas.load_encoder('fine_tuned_lm_enc')\n",
    "learn_clas.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn_clas.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_clas.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will be turning our model to mixed-precision in order to speed up the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_clas.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.242503</td>\n",
       "      <td>0.202670</td>\n",
       "      <td>0.921240</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.245878</td>\n",
       "      <td>0.232431</td>\n",
       "      <td>0.908080</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.219563</td>\n",
       "      <td>0.914240</td>\n",
       "      <td>02:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.242020</td>\n",
       "      <td>0.179243</td>\n",
       "      <td>0.933080</td>\n",
       "      <td>02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.230897</td>\n",
       "      <td>0.175639</td>\n",
       "      <td>0.935120</td>\n",
       "      <td>02:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.225465</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.934600</td>\n",
       "      <td>02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.224750</td>\n",
       "      <td>0.173133</td>\n",
       "      <td>0.935280</td>\n",
       "      <td>02:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.203338</td>\n",
       "      <td>0.175705</td>\n",
       "      <td>0.934680</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.194384</td>\n",
       "      <td>0.172137</td>\n",
       "      <td>0.936440</td>\n",
       "      <td>02:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.212245</td>\n",
       "      <td>0.184981</td>\n",
       "      <td>0.935640</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_clas.fit_one_cycle(10, 2e-2, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_clas.save('learn_clas_freezed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradual unfreezing during fine-tuning our classifier\n",
    "Fine-tuning the target classifier is the most critical part of the *transfer learning method*. Overly aggressive fine-tuning will cause catastrophic forgetting, eliminating the benefit of the information captured through language modeling; too cautious fine-tuning will lead to slow convergence (and resultant overfiting). Besides discriminative fine-tuning and triangular learning rates, **gradual unfreezing** is proposed.\n",
    "\n",
    "Jeremy et al. found that for RNN-base NLP models, by gradually unfreeze the layers from head to bottom we minimize the forgetting incurred for each *transfer learning* thus maximize our effort done in the previous training section.\n",
    "\n",
    "We first unfreeze the **last layer** and fine-tune all un-frozen layers for one epoch.\n",
    "\n",
    "Then unfreeze the **next lower frozen layer** and repeat.\n",
    "\n",
    "Until all unfrozen layers converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.222187</td>\n",
       "      <td>0.180545</td>\n",
       "      <td>0.931840</td>\n",
       "      <td>02:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.199225</td>\n",
       "      <td>0.155029</td>\n",
       "      <td>0.943760</td>\n",
       "      <td>02:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155391</td>\n",
       "      <td>0.151604</td>\n",
       "      <td>0.945680</td>\n",
       "      <td>02:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.093648</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>0.943680</td>\n",
       "      <td>02:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.058335</td>\n",
       "      <td>0.191606</td>\n",
       "      <td>0.941960</td>\n",
       "      <td>02:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_clas.freeze_to(-2)\n",
    "learn_clas.fit_one_cycle(5, slice(1e-2/(2.6**4),1e-2), moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_clas.save('learn_clas_unfreed_second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.064002</td>\n",
       "      <td>0.204184</td>\n",
       "      <td>0.940280</td>\n",
       "      <td>03:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.090188</td>\n",
       "      <td>0.196067</td>\n",
       "      <td>0.936440</td>\n",
       "      <td>03:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.051633</td>\n",
       "      <td>0.197808</td>\n",
       "      <td>0.939760</td>\n",
       "      <td>03:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.025068</td>\n",
       "      <td>0.227497</td>\n",
       "      <td>0.944320</td>\n",
       "      <td>03:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.011097</td>\n",
       "      <td>0.242228</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>03:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_clas.freeze_to(-3)\n",
    "learn_clas.fit_one_cycle(5, slice(5e-3/(2.6**4), 5e-3), moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_clas.save('learn_clas_unfreezed_third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_clas.load('learn_clas_unfreezed_third');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.017045</td>\n",
       "      <td>0.236845</td>\n",
       "      <td>0.943680</td>\n",
       "      <td>04:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.017342</td>\n",
       "      <td>0.244636</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>04:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.256718</td>\n",
       "      <td>0.944560</td>\n",
       "      <td>04:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>0.262101</td>\n",
       "      <td>0.944880</td>\n",
       "      <td>04:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013174</td>\n",
       "      <td>0.271987</td>\n",
       "      <td>0.943320</td>\n",
       "      <td>04:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.276141</td>\n",
       "      <td>0.943960</td>\n",
       "      <td>04:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>0.276535</td>\n",
       "      <td>0.944960</td>\n",
       "      <td>03:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.010663</td>\n",
       "      <td>0.292337</td>\n",
       "      <td>0.945400</td>\n",
       "      <td>04:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.945200</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.009154</td>\n",
       "      <td>0.294201</td>\n",
       "      <td>0.944960</td>\n",
       "      <td>04:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.305880</td>\n",
       "      <td>0.946320</td>\n",
       "      <td>04:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007767</td>\n",
       "      <td>0.310033</td>\n",
       "      <td>0.946040</td>\n",
       "      <td>03:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.298653</td>\n",
       "      <td>0.945640</td>\n",
       "      <td>03:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.006778</td>\n",
       "      <td>0.302350</td>\n",
       "      <td>0.945440</td>\n",
       "      <td>04:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004617</td>\n",
       "      <td>0.306093</td>\n",
       "      <td>0.946720</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_clas.unfreeze()\n",
    "learn_clas.fit_one_cycle(15, slice(4e-4/(2.6**4),5e-4), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_clas.save('learn_clas_unfreezed_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state-of-the-art result for IMDB sentimant classification result in 2017 is **94.1%**\n",
    "What we can do even better, is to build a **reversed model** as well and training a meta-learner on top of that.\n",
    "For this technique, we will be experimenting with more detail in my [sentiment analysis with non-English language]() repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
